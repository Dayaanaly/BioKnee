{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dayaanaly/BioKnee/blob/main/UnetRotula.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pynrrd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onRK6diXBoJZ",
        "outputId": "45532639-6026-4016-c0cd-aefda240c66a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynrrd\n",
            "  Downloading pynrrd-1.1.3-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from pynrrd) (1.26.4)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pynrrd) (4.12.2)\n",
            "Downloading pynrrd-1.1.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pynrrd\n",
            "Successfully installed pynrrd-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7Hp-wEaVdpq",
        "outputId": "48c676b8-8c4a-4788-839b-cc2aa540f618"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "TMuF-YxL_zqd",
        "outputId": "810fece1-a2e9-4021-9081-a18c2ef27b89"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Equipo Trifuerza/Datos/Data set grises/Dataset mascaras 2D/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-345abc86c9b7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Filtrar archivos válidos (.nrrd y .bmp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mvalid_extensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.nrrd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.bmp'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mimage_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_extensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mmask_files\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_dir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_extensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"La cantidad de imágenes y máscaras debe ser la misma\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Equipo Trifuerza/Datos/Data set grises/Dataset mascaras 2D/'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import cv2  # Para redimensionar imágenes .nrrd\n",
        "import nrrd  # Para cargar archivos .nrrd (si no lo tienes, instala con: !pip install pynrrd)\n",
        "\n",
        "# Montar Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# ================================\n",
        "# Parámetros y directorios para rótula\n",
        "# ================================\n",
        "# Directorios de imágenes y máscaras\n",
        "image_dir = '/content/drive/MyDrive/Equipo Trifuerza/Datos/Data set grises/Dataset mascaras 2D/'\n",
        "mask_dir  = '/content/drive/MyDrive/Equipo Trifuerza/Datos/Data set grises/Mascaras-2D/'\n",
        "\n",
        "# Parámetros de la imagen y segmentación\n",
        "IMG_HEIGHT = 320\n",
        "IMG_WIDTH = 224\n",
        "IMG_CHANNELS = 1      # Imágenes en escala de grises\n",
        "NUM_CLASSES = 4       # Ejemplo: Rótula, tibia, fémur, fondo\n",
        "batch_size = 8\n",
        "\n",
        "# Filtrar archivos válidos (.nrrd y .bmp)\n",
        "valid_extensions = ('.nrrd', '.bmp')\n",
        "image_files = sorted([f for f in os.listdir(image_dir) if f.lower().endswith(valid_extensions)])\n",
        "mask_files  = sorted([f for f in os.listdir(mask_dir) if f.lower().endswith(valid_extensions)])\n",
        "assert len(image_files) == len(mask_files), \"La cantidad de imágenes y máscaras debe ser la misma\"\n",
        "\n",
        "# ================================\n",
        "# Función personalizada para cargar imágenes\n",
        "# ================================\n",
        "def load_image_custom(filepath, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode='grayscale'):\n",
        "    ext = os.path.splitext(filepath)[1].lower()\n",
        "    if ext == '.nrrd':\n",
        "        # Cargar usando la librería nrrd\n",
        "        img_data, header = nrrd.read(filepath)\n",
        "        # Dependiendo del color_mode, procesamos la imagen:\n",
        "        if color_mode == 'grayscale':\n",
        "            # Si la imagen es a color, convertirla a escala de grises (promediando canales)\n",
        "            if img_data.ndim == 3 and img_data.shape[-1] > 1:\n",
        "                img_data = np.mean(img_data, axis=-1)\n",
        "            # Asegurarse de que la imagen sea 2D antes de redimensionar\n",
        "            # Redimensionar usando cv2 (cv2.resize requiere (ancho, alto))\n",
        "            img_resized = cv2.resize(img_data, (target_size[1], target_size[0]))\n",
        "            # Agregar la dimensión del canal\n",
        "            img_resized = np.expand_dims(img_resized, axis=-1)\n",
        "            return img_resized\n",
        "        elif color_mode == 'rgb':\n",
        "            # Si es color, se espera que la imagen tenga 3 canales\n",
        "            # En caso de tener una sola banda, se replicará\n",
        "            if img_data.ndim == 2:\n",
        "                img_data = np.stack((img_data,)*3, axis=-1)\n",
        "            img_resized = cv2.resize(img_data, (target_size[1], target_size[0]))\n",
        "            return img_resized\n",
        "        else:\n",
        "            raise ValueError(\"Modo de color no soportado: \" + color_mode)\n",
        "    else:\n",
        "        # Para otros formatos (.bmp), usamos load_img de Keras\n",
        "        return img_to_array(load_img(filepath, target_size=target_size, color_mode=color_mode))\n",
        "\n",
        "# ================================\n",
        "# Cargar y preprocesar imágenes y máscaras\n",
        "# ================================\n",
        "images = []\n",
        "masks = []\n",
        "\n",
        "for img_file, msk_file in zip(image_files, mask_files):\n",
        "    image_path = os.path.join(image_dir, img_file)\n",
        "    mask_path  = os.path.join(mask_dir, msk_file)\n",
        "\n",
        "    # Cargar la imagen y normalizar (valores entre 0 y 1)\n",
        "    img = load_image_custom(image_path, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode='grayscale')\n",
        "    img = img.astype('float32') / 255.0\n",
        "\n",
        "    # Cargar la máscara (se espera que contenga índices de clase)\n",
        "    mask = load_image_custom(mask_path, target_size=(IMG_HEIGHT, IMG_WIDTH), color_mode='grayscale')\n",
        "    mask = mask.astype(np.int64)\n",
        "\n",
        "    images.append(img)\n",
        "    masks.append(mask)\n",
        "\n",
        "# Convertir listas a arrays de NumPy\n",
        "images = np.array(images)\n",
        "masks = np.array(masks)\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y validación (por ejemplo, 90% train, 10% val)\n",
        "images_train, images_val, masks_train, masks_val = train_test_split(\n",
        "    images, masks, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# Crear generadores de datos con aumento (augmentation)\n",
        "# ================================\n",
        "\"\"\"data_gen_args = dict(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "image_datagen = ImageDataGenerator(**data_gen_args)\n",
        "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
        "\n",
        "seed = 42\n",
        "\n",
        "train_image_generator = image_datagen.flow(\n",
        "    images_train, batch_size=batch_size, seed=seed\n",
        ")\n",
        "train_mask_generator = mask_datagen.flow(\n",
        "    masks_train, batch_size=batch_size, seed=seed\n",
        ")\n",
        "\n",
        "val_image_generator = image_datagen.flow(\n",
        "    images_val, batch_size=batch_size, seed=seed\n",
        ")\n",
        "val_mask_generator = mask_datagen.flow(\n",
        "    masks_val, batch_size=batch_size, seed=seed\n",
        ")\n",
        "\n",
        "def train_generator():\n",
        "    while True:\n",
        "        imgs = next(train_image_generator)  # Usar next() en lugar de .next()\n",
        "        msks = next(train_mask_generator)\n",
        "        yield (imgs, msks)\n",
        "\n",
        "def val_generator():\n",
        "    while True:\n",
        "        imgs = next(val_image_generator)\n",
        "        msks = next(val_mask_generator)\n",
        "        yield (imgs, msks)\n",
        "\n",
        "\n",
        "# (Opcional) Visualizar algunas imágenes y máscaras aumentadas\n",
        "batch = next(train_generator())\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(5):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(batch[0][i].squeeze(), cmap='gray')\n",
        "    plt.title(\"Imagen Aumentada\")\n",
        "    plt.axis('off')\n",
        "    plt.subplot(2, 5, i+6)\n",
        "    plt.imshow(batch[1][i].squeeze(), cmap='gray')\n",
        "    plt.title(\"Máscara Aumentada\")\n",
        "    plt.axis('off')\n",
        "plt.show()\"\"\"\n",
        "\n",
        "# ================================\n",
        "# Definir la U-Net para segmentación multi‑clase\n",
        "# ================================\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "def U_net(input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), num_classes=NUM_CLASSES):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Bloque Down 1\n",
        "    conv1 = layers.Conv2D(64, 3, padding='same', activation='relu')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, padding='same', activation='relu')(conv1)\n",
        "    pool1 = layers.MaxPooling2D((2, 2))(conv1)\n",
        "\n",
        "    # Bloque Down 2\n",
        "    conv2 = layers.Conv2D(128, 3, padding='same', activation='relu')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, padding='same', activation='relu')(conv2)\n",
        "    pool2 = layers.MaxPooling2D((2, 2))(conv2)\n",
        "\n",
        "    # Bloque Down 3\n",
        "    conv3 = layers.Conv2D(256, 3, padding='same', activation='relu')(pool2)\n",
        "    conv3 = layers.Conv2D(256, 3, padding='same', activation='relu')(conv3)\n",
        "    pool3 = layers.MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "    # Bloque intermedio\n",
        "    conv4 = layers.Conv2D(512, 3, padding='same', activation='relu')(pool3)\n",
        "    conv4 = layers.Conv2D(512, 3, padding='same', activation='relu')(conv4)\n",
        "\n",
        "    # Up 1\n",
        "    up1 = layers.UpSampling2D((2, 2))(conv4)\n",
        "    concat1 = layers.concatenate([up1, conv3])\n",
        "    conv5 = layers.Conv2D(256, 3, padding='same', activation='relu')(concat1)\n",
        "    conv5 = layers.Conv2D(256, 3, padding='same', activation='relu')(conv5)\n",
        "\n",
        "    # Up 2\n",
        "    up2 = layers.UpSampling2D((2, 2))(conv5)\n",
        "    concat2 = layers.concatenate([up2, conv2])\n",
        "    conv6 = layers.Conv2D(128, 3, padding='same', activation='relu')(concat2)\n",
        "    conv6 = layers.Conv2D(128, 3, padding='same', activation='relu')(conv6)\n",
        "\n",
        "    # Up 3\n",
        "    up3 = layers.UpSampling2D((2, 2))(conv6)\n",
        "    concat3 = layers.concatenate([up3, conv1])\n",
        "    conv7 = layers.Conv2D(64, 3, padding='same', activation='relu')(concat3)\n",
        "    conv7 = layers.Conv2D(64, 3, padding='same', activation='relu')(conv7)\n",
        "\n",
        "    # Capa de salida: NUM_CLASSES canales con activación softmax\n",
        "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(conv7)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = U_net()\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "steps_per_epoch = len(images_train) // batch_size\n",
        "validation_steps = len(images_val) // batch_size\n",
        "\n",
        "# ================================\n",
        "# Entrenamiento\n",
        "# ================================\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    #images_train, masks_train,\n",
        "    #batch_size=batch_size,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_generator,#(images_val,masks_val)\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=10 # Ajusta el número de épocas según tus necesidades\n",
        ")\n",
        "\n",
        "# Guardar el modelo entrenado\n",
        "model.save('Prediccion_rotula.h5')\n",
        "\n",
        "# ================================\n",
        "# Graficar curvas de entrenamiento\n",
        "# ================================\n",
        "\n",
        "# Visualizar algunas imágenes y máscaras aumentadas en formato vertical\n",
        "batch = next(train_generator())\n",
        "plt.figure(figsize=(6, 10))  # Ajustar tamaño para alineación vertical\n",
        "\n",
        "for i in range(5):  # Mostrar 5 ejemplos\n",
        "    plt.subplot(5, 2, 2*i+1)  # Columna izquierda: Imagen aumentada\n",
        "    plt.imshow(batch[0][i].squeeze(), cmap='gray')\n",
        "    plt.title(f\"Imagen {i+1}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(5, 2, 2*i+2)  # Columna derecha: Máscara aumentada\n",
        "    plt.imshow(batch[1][i].squeeze(), cmap='gray')\n",
        "    plt.title(f\"Máscara {i+1}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Graficas del valor de pérdida\n",
        "plt.plot(history.history['loss'], label='Pérdida Entrenamiento')\n",
        "plt.plot(history.history['val_loss'], label='Pérdida Validación')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida')\n",
        "plt.title('Evolución de la pérdida')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='Exactitud Entrenamiento')\n",
        "plt.plot(history.history['val_accuracy'], label='Exactitud Validación')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Exactitud')\n",
        "plt.title('Evolución de la exactitud')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}